{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import utils as U\n",
    "#import opts\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of the architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> to add in an external file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to specify\n",
    "#bashCommand = \"python esc_gen.py . \"\n",
    "#process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "#output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize hyperparameters\n",
    "        \"\"\"          \n",
    "        # General settings\n",
    "        self.data='.'\n",
    "        #self.data='/home/dl5/team10/Perla/data/ESC-50-master/audio_resampled'\n",
    "        self.save=None #'Directory to save the results'\n",
    "    \n",
    "        # Dataset details\n",
    "        self.dataset = 'esc50'\n",
    "        self.nClasses = 50 #classes are encoding from 0 to 49\n",
    "        self.nFolds = 5\n",
    "        self.splits = range(1, self.nFolds + 1)\n",
    "        self.nCrops=10\n",
    "    \n",
    "        # Model details\n",
    "        \n",
    "        #Define hyperparameters\n",
    "        self.netType = 'envnet'\n",
    "        self.fs = 16000\n",
    "        self.inputLength = 24014\n",
    "        self.nEpochs =600\n",
    "        self.LR=0.005\n",
    "        self.momentum=0.9\n",
    "        self.weightDecay=5e-4\n",
    "        self.momentum=0.9\n",
    "        self.batchSize=64\n",
    "        self.optimizer='nesterov'\n",
    "        \n",
    "        \n",
    "        self.schedule=[0.5, 0.75]\n",
    "        self.warmup=0\n",
    "    \n",
    "#         if self.save != 'None' and not os.path.isdir(self.save):\n",
    "#             os.makedirs(self.save)\n",
    "    \n",
    "        self.display_info()\n",
    "    \n",
    "    def display_info(opt):\n",
    "        print('+------------------------------+')\n",
    "        print('| Sound classification')\n",
    "        print('+------------------------------+')\n",
    "        print('| dataset  : {}'.format(opt.dataset))\n",
    "        print('| netType  : {}'.format(opt.netType))\n",
    "        print('| nEpochs  : {}'.format(opt.nEpochs))\n",
    "        print('| LRInit   : {}'.format(opt.LR))\n",
    "        print('| schedule : {}'.format(opt.schedule))\n",
    "        print('| warmup   : {}'.format(opt.warmup))\n",
    "        print('| batchSize: {}'.format(opt.batchSize))\n",
    "        print('| optimize: {}'.format(opt.optimizer))\n",
    "        print('+------------------------------+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "| Sound classification\n",
      "+------------------------------+\n",
      "| dataset  : esc50\n",
      "| netType  : envnet\n",
      "| nEpochs  : 600\n",
      "| LRInit   : 0.005\n",
      "| schedule : [0.5, 0.75]\n",
      "| warmup   : 0\n",
      "| batchSize: 64\n",
      "| optimize: nesterov\n",
      "+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "param = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Pre-processing the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels,param):\n",
    "    b = np.zeros((len(labels), param.nClasses))\n",
    "    b[np.arange(len(labels)), labels] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(sounds, size=param.inputLength):\n",
    "    cropped_sounds=[]\n",
    "    for s in sounds:\n",
    "        org_size = len(s)\n",
    "        start = random.randint(0, org_size - size)\n",
    "        cropped_sounds.append(s[start: start + size])\n",
    "    return cropped_sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sounds, pad=param.inputLength // 2):\n",
    "    return [np.pad(s, pad, 'constant') for s in sounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sounds, factor=32768.0):\n",
    "    return [s/factor for s in sounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing phase\n",
    "def multi_crop(sounds,input_length=param.inputLength, n_crops=param.nCrops):\n",
    "    multi_cropped_sounds=[]\n",
    "    for s in sounds:\n",
    "        stride = (len(s) - input_length) // (n_crops - 1)\n",
    "        multi_cropped_sound = [s[stride * i: stride * i + input_length] for i in range(n_crops)]\n",
    "        multi_cropped_sounds.append(np.array(multi_cropped_sound))\n",
    "    return multi_cropped_sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : implement a function of pre-processing (prototype in ESC_vs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(param, split):\n",
    "    dataset = np.load(os.path.join(param.data,'wav16.npz'))\n",
    "    # Split to train and val\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    val_sounds = []\n",
    "    val_labels = []\n",
    "    for i in range(1, param.nFolds + 1):\n",
    "        sounds = dataset['fold{}'.format(i)].item()['sounds']\n",
    "        labels = dataset['fold{}'.format(i)].item()['labels']\n",
    "        if i == split:\n",
    "            #val_sounds.extend(preprocess(param,sounds,False))\n",
    "            val_sounds.extend(sounds)\n",
    "            val_labels.extend(labels)\n",
    "        else:\n",
    "            #train_sounds.extend(preprocess(param,sounds,True))\n",
    "            train_sounds.extend(sounds)\n",
    "            train_labels.extend(labels)\n",
    "            \n",
    "    train_sounds=normalize(random_crop(padding(train_sounds)))        \n",
    "    train_labels= one_hot_encoding(train_labels,param)\n",
    "    \n",
    "    val_sounds=normalize(random_crop(padding(val_sounds)))\n",
    "    val_labels= one_hot_encoding(val_labels,param)\n",
    "    \n",
    "    return train_sounds,train_labels, val_sounds,val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with only one split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sounds,train_labels,val_sounds,val_labels=setup(param,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sounds=np.asarray(train_sounds,dtype=np.float32)[:,None,:,None]\n",
    "train_labels=np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator setup\n",
    "#train_iter = tf.data.Dataset.from_tensor_slices((np.asarray(train_sounds),np.asarray(train_labels))).repeat().batch(param.batchSize)\n",
    "\n",
    "#train_iter = tf.data.Dataset.from_tensor_slices((np.asarray(train_sounds,dtype=np.float32)[:,None,:,None],np.asarray(train_labels))).shuffle(True).repeat().batch(param.batchSize)\n",
    "#val_iter = tf.data.Dataset.from_tensor_slices((np.asarray(val_sounds,dtype=np.float32)[:,None,:,None],np.asarray(val_labels))).shuffle(False).batch(param.batchSize// param.nCrops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of EnvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : create a class class EnvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs\n",
    "def add_placeholders():\n",
    "    \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "    # shape = (batch size, max length of sentence in batch)\n",
    "    #self.word_ids = tf.placeholder(tf.int32, shape=[None, None],name=\"word_ids\")\n",
    "    X = tf.placeholder(tf.float32,[None, 1,param.inputLength,1],name=\"input_X\")\n",
    "    Y = tf.placeholder(tf.int32, [None, param.nClasses], name=\"input_Y\")\n",
    "    \n",
    "    # hyperparameters\n",
    "    #dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "    #               name=\"dropout\")\n",
    "    #lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "    #                name=\"lr\")\n",
    "    #return dropout, lr\n",
    "    return X,Y\n",
    "\n",
    "def conv_bn_relu(inputTensor,out_channels,ksize,pad,initialW,bias,name,stride=(1,1)):\n",
    "    with tf.variable_scope(name):\n",
    "        conv = tf.layers.conv2d(inputs=inputTensor,filters=out_channels, kernel_size=ksize,padding=pad,strides=stride,kernel_initializer=initialW,use_bias=bias) #name to configure\n",
    "        bn = tf.layers.batch_normalization(conv)\n",
    "        relu=tf.nn.relu(bn)\n",
    "    return  relu\n",
    "\n",
    "def net(x,n_classes):\n",
    "    conv1=conv_bn_relu(inputTensor=x, out_channels=40, ksize=[1,8],pad='valid',initialW=tf.initializers.truncated_normal,bias=False,name='conv1')\n",
    "    conv2=conv_bn_relu(inputTensor=conv1, out_channels=40, ksize=[1,8],pad='valid',initialW=tf.initializers.truncated_normal,bias=False,name='conv2')\n",
    "    pool2=tf.layers.max_pooling2d(conv2,pool_size=[1,160],strides=(1,160),padding='valid',name='pool2')\n",
    "    #by using data_format channels last : we have : batch*N_h*N_w*N_c :(batch*1*150*40)\n",
    "    x_perm=tf.transpose(pool2, perm=[0,3,2,1])\n",
    "    conv3=conv_bn_relu(inputTensor=x_perm, out_channels=50, ksize=[8,13],pad='valid',initialW=tf.initializers.truncated_normal,bias=False,name='conv3')\n",
    "    pool3=tf.layers.max_pooling2d(conv3,pool_size=[3,3],strides=(3,3),padding='valid',name='pool3')\n",
    "    conv4=conv_bn_relu(inputTensor=pool3, out_channels=50, ksize=[1,5],pad='valid',initialW=tf.initializers.truncated_normal,bias=False,name='conv4')\n",
    "    pool4=tf.layers.max_pooling2d(conv3,pool_size=[1,3],strides=(1,3),padding='valid',name='pool4')\n",
    "\n",
    "    #besoin éventuel de vectoriser\n",
    "    #flatten_pool4_out = tf.contrib.layers.flatten(pool4)\n",
    "    sha=pool4.get_shape().as_list()\n",
    "    flatten_pool4_out=tf.reshape(pool4,[-1,np.prod(sha[1:])])\n",
    "    \n",
    "    fc5=tf.layers.dense(flatten_pool4_out,4096,name='fc5')\n",
    "    #fc5=tf.layers.dense(pool4,4096,name='fc5')\n",
    "    fc5= tf.nn.dropout(tf.nn.relu(fc5),keep_prob=0.5)\n",
    "    fc6=tf.layers.dense(fc5, 4096,name='fc6')\n",
    "    fc6= tf.nn.dropout(tf.nn.relu(fc6),keep_prob=0.5)\n",
    "    fc7=tf.layers.dense(fc6, n_classes,name='fc7')\n",
    "\n",
    "    return fc7\n",
    "\n",
    "def add_loss_op(logits,labels):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    return loss\n",
    "\n",
    "def add_train_op(lr_method, lr,momentum, loss):\n",
    "    \"\"\"Defines self.train_op that performs an update on a batch\n",
    "\n",
    "    Args:\n",
    "        lr_method: (string) sgd method, for example \"adam\"\n",
    "        lr: (tf.placeholder) tf.float32, learning rate\n",
    "        loss: (tensor) tf.float32 loss to minimize\n",
    "        clip: (python float) clipping of gradient. If < 0, no clipping\n",
    "\n",
    "    \"\"\"\n",
    "    _lr_m = lr_method.lower() # lower to make sure\n",
    "\n",
    "    with tf.variable_scope(\"train_step\"):\n",
    "        if _lr_m == 'adam': # sgd method\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "        elif _lr_m == 'adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(lr)\n",
    "        elif _lr_m == 'sgd':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        elif _lr_m == 'rmsprop':\n",
    "            optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "        elif _lr_m == 'nesterov':\n",
    "            optimizer = tf.train.MomentumOptimizer(lr,momentum=momentum,use_nesterov=True)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        #optimizer.add_hook(chainer.optimizer.WeightDecay(opt.weightDecay))\n",
    "        #update hook function for regularization called right after the gradient computation\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(param):\n",
    "    x,y=add_placeholders()   \n",
    "    logits=net(x,param.nClasses)\n",
    "    loss=add_loss_op(logits,y)\n",
    "    train_op=add_train_op(param.optimizer,param.LR,param.momentum,loss)\n",
    "    #prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "    #correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "    return x,y,loss, train_op\n",
    "    #, prediction,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#iter = train_iter.make_one_shot_iterator()\n",
    "#x, y = iter.get_next()\n",
    "#loss, train_op = build(x,y,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "#    for i in range(param.nEpochs):\n",
    "#        _, loss_value = sess.run([train_op, loss])\n",
    "#        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 10/600 | Train: LR 0.005  Loss nan\n",
      "\n",
      "| Epoch: 20/600 | Train: LR 0.005  Loss nan\n",
      "\n",
      "| Epoch: 30/600 | Train: LR 0.005  Loss nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x,y,loss, train_op=build(param)\n",
    "init =tf.global_variables_initializer()\n",
    "#the model\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    with tf.device(\"/device:GPU:0\"):\n",
    "        sess.run(init)\n",
    "        #x,y,loss, train_op, prediction,accuracy=build(param)*        \n",
    "        #Run epoch\n",
    "        for epoch in range(1, param.nEpochs + 1):\n",
    "            n_batches = (param.fs // param.batchSize) + 1 \n",
    "            for i in range(n_batches):\n",
    "                if i==n_batches-1 :\n",
    "                    if param.fs-1>i*param.batchSize-1:\n",
    "                        batch_x=train_sounds[i*param.batchSize:1]\n",
    "                        batch_y=train_labels[i*param.batchSize:]\n",
    "                    else :\n",
    "                        continue\n",
    "                else:\n",
    "                    batch_x=train_sounds[i*param.batchSize:(i+1)*param.batchSize-1]\n",
    "                    batch_y=train_labels[i*param.batchSize:(i+1)*param.batchSize-1,]\n",
    "\n",
    "                _, train_loss = sess.run((train_op, loss),feed_dict={x: batch_x, y: batch_y})\n",
    "                #val_top1 = trainer.val() \n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "\n",
    "                print('| Epoch: {}/{} | Train: LR {}  Loss {:.3f}\\n'.format(\n",
    "                        epoch, param.nEpochs, param.LR, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : Execute K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sounds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sounds[0*param.batchSize:1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in opt.splits:\n",
    "        print('+-- Split {} --+'.format(split))\n",
    "#         train(opt, split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
